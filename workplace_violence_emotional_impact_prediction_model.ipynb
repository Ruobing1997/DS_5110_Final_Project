{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9edd4f88-87d2-4392-a6a2-f4db2c661d08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imbalanced-learn in /Users/lilysong/miniconda3/lib/python3.12/site-packages (0.13.0)\n",
      "Requirement already satisfied: numpy<3,>=1.24.3 in /Users/lilysong/miniconda3/lib/python3.12/site-packages (from imbalanced-learn) (2.2.1)\n",
      "Requirement already satisfied: scipy<2,>=1.10.1 in /Users/lilysong/miniconda3/lib/python3.12/site-packages (from imbalanced-learn) (1.15.2)\n",
      "Requirement already satisfied: scikit-learn<2,>=1.3.2 in /Users/lilysong/miniconda3/lib/python3.12/site-packages (from imbalanced-learn) (1.6.1)\n",
      "Requirement already satisfied: sklearn-compat<1,>=0.1 in /Users/lilysong/miniconda3/lib/python3.12/site-packages (from imbalanced-learn) (0.1.3)\n",
      "Requirement already satisfied: joblib<2,>=1.1.1 in /Users/lilysong/miniconda3/lib/python3.12/site-packages (from imbalanced-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in /Users/lilysong/miniconda3/lib/python3.12/site-packages (from imbalanced-learn) (3.5.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fed63ca1-5fec-4ae8-8108-7b55312b2d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: shap in /Users/lilysong/miniconda3/lib/python3.12/site-packages (0.47.1)\n",
      "Requirement already satisfied: imbalanced-learn in /Users/lilysong/miniconda3/lib/python3.12/site-packages (0.13.0)\n",
      "Requirement already satisfied: numpy in /Users/lilysong/miniconda3/lib/python3.12/site-packages (from shap) (2.2.1)\n",
      "Requirement already satisfied: scipy in /Users/lilysong/miniconda3/lib/python3.12/site-packages (from shap) (1.15.2)\n",
      "Requirement already satisfied: scikit-learn in /Users/lilysong/miniconda3/lib/python3.12/site-packages (from shap) (1.6.1)\n",
      "Requirement already satisfied: pandas in /Users/lilysong/miniconda3/lib/python3.12/site-packages (from shap) (2.2.3)\n",
      "Requirement already satisfied: tqdm>=4.27.0 in /Users/lilysong/miniconda3/lib/python3.12/site-packages (from shap) (4.66.5)\n",
      "Requirement already satisfied: packaging>20.9 in /Users/lilysong/miniconda3/lib/python3.12/site-packages (from shap) (24.1)\n",
      "Requirement already satisfied: slicer==0.0.8 in /Users/lilysong/miniconda3/lib/python3.12/site-packages (from shap) (0.0.8)\n",
      "Requirement already satisfied: numba>=0.54 in /Users/lilysong/miniconda3/lib/python3.12/site-packages (from shap) (0.61.2)\n",
      "Requirement already satisfied: cloudpickle in /Users/lilysong/miniconda3/lib/python3.12/site-packages (from shap) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions in /Users/lilysong/miniconda3/lib/python3.12/site-packages (from shap) (4.12.2)\n",
      "Requirement already satisfied: sklearn-compat<1,>=0.1 in /Users/lilysong/miniconda3/lib/python3.12/site-packages (from imbalanced-learn) (0.1.3)\n",
      "Requirement already satisfied: joblib<2,>=1.1.1 in /Users/lilysong/miniconda3/lib/python3.12/site-packages (from imbalanced-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in /Users/lilysong/miniconda3/lib/python3.12/site-packages (from imbalanced-learn) (3.5.0)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /Users/lilysong/miniconda3/lib/python3.12/site-packages (from numba>=0.54->shap) (0.44.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/lilysong/miniconda3/lib/python3.12/site-packages (from pandas->shap) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/lilysong/miniconda3/lib/python3.12/site-packages (from pandas->shap) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/lilysong/miniconda3/lib/python3.12/site-packages (from pandas->shap) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/lilysong/miniconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->shap) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install shap imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "215cca94-c570-4d71-bdfc-69870b9e0e36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing time features...\n",
      "Successfully extracted time features for 1323 records\n",
      "\n",
      "\n",
      "Performing Extended EDA Analysis...\n",
      "Analyzing violence patterns by shift and profession...\n",
      "Analyzing relationship between violence type and emotional impact...\n",
      "Generating text embedding visualization...\n",
      "Analyzing departmental patterns by time...\n",
      "Analyzing violence patterns by time of day...\n",
      "Extended EDA analysis complete!\n",
      "Preprocessing data...\n",
      "Target classes: ['Mild' 'Moderate' 'Severe']\n",
      "TF-IDF extracted 300 text features\n",
      "Combining features...\n",
      "Added time features to the model\n",
      "Final feature matrix shape: (1141, 740)\n",
      "Training set shape after SMOTE: (1914, 740)\n",
      "Training and evaluating models...\n",
      "\n",
      "Training Logistic Regression model...\n",
      "Logistic Regression - Cross-Validation Score (F1 weighted): 0.9126 ± 0.0221\n",
      "\n",
      "Logistic Regression - Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Mild       0.50      0.41      0.45        17\n",
      "    Moderate       0.83      0.82      0.82       160\n",
      "      Severe       0.49      0.54      0.51        52\n",
      "\n",
      "    accuracy                           0.72       229\n",
      "   macro avg       0.61      0.59      0.60       229\n",
      "weighted avg       0.73      0.72      0.73       229\n",
      "\n",
      "Generating ROC curves for Logistic Regression...\n",
      "\n",
      "Training Random Forest model...\n",
      "Random Forest - Cross-Validation Score (F1 weighted): 0.8784 ± 0.0862\n",
      "\n",
      "Random Forest - Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Mild       0.58      0.41      0.48        17\n",
      "    Moderate       0.85      0.89      0.87       160\n",
      "      Severe       0.56      0.52      0.54        52\n",
      "\n",
      "    accuracy                           0.77       229\n",
      "   macro avg       0.66      0.61      0.63       229\n",
      "weighted avg       0.76      0.77      0.77       229\n",
      "\n",
      "Generating ROC curves for Random Forest...\n",
      "\n",
      "Training SVM model...\n",
      "SVM - Cross-Validation Score (F1 weighted): 0.8963 ± 0.0338\n",
      "\n",
      "SVM - Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Mild       0.45      0.29      0.36        17\n",
      "    Moderate       0.78      0.84      0.81       160\n",
      "      Severe       0.45      0.40      0.42        52\n",
      "\n",
      "    accuracy                           0.70       229\n",
      "   macro avg       0.56      0.51      0.53       229\n",
      "weighted avg       0.68      0.70      0.69       229\n",
      "\n",
      "Generating ROC curves for SVM...\n",
      "Analyzing feature importance...\n",
      "Generated time feature importance plot\n",
      "Performing SHAP analysis...\n",
      "Generated SHAP analysis plot for Mild class\n",
      "Generated SHAP analysis plot for Moderate class\n",
      "Generated SHAP analysis plot for Severe class\n",
      "\n",
      "\n",
      "Model Comparison Summary (Weighted F1, Precision, Recall):\n",
      "Logistic Regression: F1=0.73, Precision=0.73, Recall=0.72\n",
      "Random Forest: F1=0.77, Precision=0.76, Recall=0.77\n",
      "SVM: F1=0.69, Precision=0.68, Recall=0.70\n",
      "Generated model comparison chart\n",
      "Performing KMeans clustering analysis...\n",
      "\n",
      "Top 3 'victim_profession' values per cluster:\n",
      "Cluster  Cluster  victim_profession\n",
      "0        0        other                239\n",
      "                  nurse                 79\n",
      "                  security              23\n",
      "1        1        nurse                247\n",
      "                  tech                 140\n",
      "                  security              79\n",
      "2        2        other                284\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Top 3 'department' values per cluster:\n",
      "Cluster  Cluster  department          \n",
      "0        0        other                   193\n",
      "                  emergency department     38\n",
      "                  Emergency Department     36\n",
      "1        1        emergency department    113\n",
      "                  katahdin 2 south         93\n",
      "                  other                    32\n",
      "2        2        other                    85\n",
      "                  Emergency Department     51\n",
      "                  bc                       42\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Top 3 'violence_type' values per cluster:\n",
      "Cluster  Cluster  violence_type\n",
      "0        0        verbal           380\n",
      "                  other              1\n",
      "1        1        physical         433\n",
      "                  both              21\n",
      "                  other             16\n",
      "2        2        physical         238\n",
      "                  other             40\n",
      "                  threat             6\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "Time Feature Analysis:\n",
      "=======================\n",
      "Hours with highest severe incident rates:\n",
      "Hour 0: 25.71%\n",
      "Hour 14: 5.56%\n",
      "Hour 8: 5.45%\n",
      "\n",
      "Weekday vs. Weekend incident severity comparison:\n",
      "Weekday severity distribution:\n",
      "emotional_impact_ML\n",
      "Moderate    0.716157\n",
      "Severe      0.220524\n",
      "Mild        0.063319\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Weekend severity distribution:\n",
      "emotional_impact_ML\n",
      "Moderate    0.631818\n",
      "Severe      0.254545\n",
      "Mild        0.113636\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Analysis complete! All charts and results have been saved to the 'cleaned_data/final/imgs/' directory.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "# Expanded imports for ROC curves and cross-validation\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, ConfusionMatrixDisplay, \n",
    "                           roc_curve, auc, roc_auc_score)\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "import shap\n",
    "import os\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(\"cleaned_data/final/imgs\", exist_ok=True)\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"cleaned_data/final/BERT/all_merged_labeled_contextual.csv\")\n",
    "\n",
    "# 1. Process time-based features (new addition)\n",
    "print(\"Processing time features...\")\n",
    "# Convert event_time to datetime type\n",
    "df['event_time'] = pd.to_datetime(df['event_time'], errors='coerce')\n",
    "\n",
    "# Extract time features\n",
    "time_features = pd.DataFrame(index=df.index)\n",
    "# Only extract features for non-null dates\n",
    "valid_dates = df['event_time'].notna()\n",
    "if valid_dates.any():\n",
    "    time_features.loc[valid_dates, 'hour'] = df.loc[valid_dates, 'event_time'].dt.hour\n",
    "    time_features.loc[valid_dates, 'day_of_week'] = df.loc[valid_dates, 'event_time'].dt.dayofweek\n",
    "    time_features.loc[valid_dates, 'month'] = df.loc[valid_dates, 'event_time'].dt.month\n",
    "    time_features.loc[valid_dates, 'is_weekend'] = df.loc[valid_dates, 'event_time'].dt.dayofweek >= 5\n",
    "    \n",
    "    # Create day_night feature (7am-7pm is day)\n",
    "    valid_indices = df.index[valid_dates]\n",
    "    day_hour_mask = (df.loc[valid_dates, 'event_time'].dt.hour >= 7) & (df.loc[valid_dates, 'event_time'].dt.hour < 19)\n",
    "    day_indices = valid_indices[day_hour_mask]\n",
    "    night_indices = valid_indices[~day_hour_mask]\n",
    "\n",
    "    time_features.loc[day_indices, 'day_night'] = 'day'\n",
    "    time_features.loc[night_indices, 'day_night'] = 'night'\n",
    "    \n",
    "    # Fill NaN values with appropriate default values\n",
    "    time_features = time_features.fillna(-1)  # -1 represents unknown time\n",
    "    \n",
    "    # Visualize event distribution across different time periods\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Plot events by hour of day\n",
    "    plt.subplot(1, 2, 1)\n",
    "    hour_counts = time_features.loc[valid_dates, 'hour'].value_counts().sort_index()\n",
    "    sns.barplot(x=hour_counts.index, y=hour_counts.values)\n",
    "    plt.title('Events by Hour of Day')\n",
    "    plt.xlabel('Hour')\n",
    "    plt.ylabel('Event Count')\n",
    "    \n",
    "    # Plot events by day of week\n",
    "    plt.subplot(1, 2, 2)\n",
    "    days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "    day_counts = time_features.loc[valid_dates, 'day_of_week'].value_counts().sort_index()\n",
    "\n",
    "    # Convert float indices to integers before using them as list indices\n",
    "    sns.barplot(x=[days[int(i)] for i in day_counts.index], y=day_counts.values)\n",
    "    plt.title('Events by Day of Week')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"cleaned_data/final/imgs/time_feature_distribution.png\")\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"Successfully extracted time features for {valid_dates.sum()} records\")\n",
    "else:\n",
    "    print(\"Warning: No valid time data available\")\n",
    "\n",
    "# One-hot encode categorical variables\n",
    "if 'day_night' in time_features.columns:\n",
    "    time_features = pd.get_dummies(time_features, columns=['day_night'])\n",
    "\n",
    "# Extended EDA Analysis\n",
    "# Add this code section after the time feature extraction but before model training\n",
    "\n",
    "print(\"\\n\\nPerforming Extended EDA Analysis...\")\n",
    "\n",
    "# Create a directory for additional EDA visualizations\n",
    "os.makedirs(\"cleaned_data/final/imgs/extended_eda\", exist_ok=True)\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# 1. Night Shift/Day Shift Violence Analysis by Role\n",
    "# -----------------------------------------------------------------------\n",
    "print(\"Analyzing violence patterns by shift and profession...\")\n",
    "\n",
    "# Create a combined dataframe with time features for analysis\n",
    "if 'day_night' in time_features.columns and valid_dates.any():\n",
    "    # We need to recreate day_night as a string column (not one-hot encoded)\n",
    "    role_time_df = df.join(time_features)\n",
    "    \n",
    "    # Get top 5 most common victim professions for readability\n",
    "    top_professions = df['victim_profession'].value_counts().nlargest(5).index.tolist()\n",
    "    \n",
    "    # Filter for only top professions and valid time data\n",
    "    role_time_filtered = role_time_df[\n",
    "        role_time_df['victim_profession'].isin(top_professions) & \n",
    "        valid_dates\n",
    "    ]\n",
    "    \n",
    "    # Plot count by profession and day/night\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # First subplot: absolute counts\n",
    "    plt.subplot(2, 1, 1)\n",
    "    shift_count = sns.countplot(data=role_time_filtered, \n",
    "                               x='victim_profession', \n",
    "                               hue='day_night')\n",
    "    plt.title('Incident Counts by Profession and Shift')\n",
    "    plt.xlabel('Victim Profession')\n",
    "    plt.ylabel('Number of Incidents')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # Second subplot: proportions within each profession\n",
    "    plt.subplot(2, 1, 2)\n",
    "    \n",
    "    # Calculate proportions of day vs night incidents for each profession\n",
    "    prof_shift_props = pd.crosstab(\n",
    "        role_time_filtered['victim_profession'], \n",
    "        role_time_filtered['day_night'],\n",
    "        normalize='index'\n",
    "    )\n",
    "    \n",
    "    # Plot as stacked bars\n",
    "    prof_shift_props.plot(kind='bar', stacked=True, figsize=(12, 6))\n",
    "    plt.title('Proportion of Day vs Night Incidents by Profession')\n",
    "    plt.xlabel('Victim Profession')\n",
    "    plt.ylabel('Proportion of Incidents')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"cleaned_data/final/imgs/extended_eda/profession_by_shift.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    # Additional analysis: Calculate incident rate per shift by profession\n",
    "    print(\"\\nDay vs Night Shift Incident Rates by Profession:\")\n",
    "    shift_rates = pd.crosstab(role_time_filtered['victim_profession'], \n",
    "                             role_time_filtered['day_night'])\n",
    "    \n",
    "    # Calculate ratio of night to day incidents (higher = more night incidents)\n",
    "    if 'night' in shift_rates.columns and 'day' in shift_rates.columns:\n",
    "        shift_rates['night_day_ratio'] = shift_rates['night'] / shift_rates['day']\n",
    "        print(shift_rates.sort_values('night_day_ratio', ascending=False))\n",
    "        \n",
    "        # Plot the night/day ratio\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.barplot(x=shift_rates.index, y=shift_rates['night_day_ratio'])\n",
    "        plt.axhline(y=1.0, color='r', linestyle='--', alpha=0.7)  # Reference line where night=day\n",
    "        plt.title('Night to Day Incident Ratio by Profession')\n",
    "        plt.xlabel('Victim Profession')\n",
    "        plt.ylabel('Night/Day Incident Ratio')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"cleaned_data/final/imgs/extended_eda/night_day_ratio.png\")\n",
    "        plt.close()\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# 2. Violence Type and Emotional Impact Analysis\n",
    "# -----------------------------------------------------------------------\n",
    "print(\"Analyzing relationship between violence type and emotional impact...\")\n",
    "\n",
    "# Check if we have the necessary columns\n",
    "if all(col in df.columns for col in ['violence_type', 'emotional_impact_ML']):\n",
    "    # Create cross-tabulation\n",
    "    violence_impact = pd.crosstab(\n",
    "        df['violence_type'], \n",
    "        df['emotional_impact_ML'],\n",
    "        normalize='index'  # Normalize by row to get proportions within each violence type\n",
    "    )\n",
    "    \n",
    "    # Plot heatmap\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.heatmap(violence_impact, annot=True, cmap='YlOrRd', fmt='.2%')\n",
    "    plt.title('Emotional Impact by Violence Type')\n",
    "    plt.ylabel('Violence Type')\n",
    "    plt.xlabel('Emotional Impact')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"cleaned_data/final/imgs/extended_eda/violence_emotional_impact.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot stacked bar chart\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    violence_impact.plot(kind='bar', stacked=True)\n",
    "    plt.title('Distribution of Emotional Impact by Violence Type')\n",
    "    plt.xlabel('Violence Type')\n",
    "    plt.ylabel('Proportion')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"cleaned_data/final/imgs/extended_eda/violence_impact_bars.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    # If 'days_missed' or similar column exists, create boxplot\n",
    "    if 'days_missed' in df.columns:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.boxplot(data=df, x='violence_type', y='days_missed')\n",
    "        plt.title('Days Missed by Violence Type')\n",
    "        plt.xlabel('Violence Type')\n",
    "        plt.ylabel('Days Missed')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"cleaned_data/final/imgs/extended_eda/violence_days_missed.png\")\n",
    "        plt.close()\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# 3. Text Embedding Visualization using t-SNE\n",
    "# -----------------------------------------------------------------------\n",
    "print(\"Generating text embedding visualization...\")\n",
    "\n",
    "# We've already created TF-IDF vectors for the assault descriptions\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Sample a subset of the text data to make visualization manageable\n",
    "max_samples = min(500, X_text.shape[0])  # Use at most 500 samples\n",
    "indices = np.random.choice(X_text.shape[0], max_samples, replace=False)\n",
    "\n",
    "# Get the subset of text vectors and corresponding labels\n",
    "X_text_sample = X_text[indices]\n",
    "y_sample = df.iloc[indices]['emotional_impact_ML'].values\n",
    "\n",
    "# Apply t-SNE to reduce to 2D\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "X_text_tsne = tsne.fit_transform(X_text_sample.toarray())\n",
    "\n",
    "# Create a DataFrame for plotting\n",
    "tsne_df = pd.DataFrame({\n",
    "    'x': X_text_tsne[:, 0],\n",
    "    'y': X_text_tsne[:, 1],\n",
    "    'impact': y_sample,\n",
    "    'violence_type': df.iloc[indices]['violence_type'].values if 'violence_type' in df.columns else ['Unknown'] * max_samples\n",
    "})\n",
    "\n",
    "# Plot by emotional impact\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.subplot(2, 1, 1)\n",
    "sns.scatterplot(data=tsne_df, x='x', y='y', hue='impact', palette='viridis', s=50, alpha=0.7)\n",
    "plt.title('t-SNE Visualization of Assault Descriptions by Emotional Impact')\n",
    "plt.xlabel('t-SNE Component 1')\n",
    "plt.ylabel('t-SNE Component 2')\n",
    "plt.legend(title='Emotional Impact')\n",
    "\n",
    "# Plot by violence type\n",
    "plt.subplot(2, 1, 2)\n",
    "sns.scatterplot(data=tsne_df, x='x', y='y', hue='violence_type', palette='Set2', s=50, alpha=0.7)\n",
    "plt.title('t-SNE Visualization of Assault Descriptions by Violence Type')\n",
    "plt.xlabel('t-SNE Component 1')\n",
    "plt.ylabel('t-SNE Component 2')\n",
    "plt.legend(title='Violence Type')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"cleaned_data/final/imgs/extended_eda/text_embedding_tsne.png\")\n",
    "plt.close()\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# 4. Department and Time Analysis\n",
    "# -----------------------------------------------------------------------\n",
    "print(\"Analyzing departmental patterns by time...\")\n",
    "\n",
    "if 'department' in df.columns and 'hour' in time_features.columns and valid_dates.any():\n",
    "    # Get top departments by incident count\n",
    "    top_depts = df['department'].value_counts().nlargest(5).index.tolist()\n",
    "    \n",
    "    # Create a dataframe with both department and hour information\n",
    "    dept_time_df = df.join(time_features).dropna(subset=['department', 'hour'])\n",
    "    dept_time_filtered = dept_time_df[dept_time_df['department'].isin(top_depts)]\n",
    "    \n",
    "    # Plot incident distribution by hour for each department\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # Group by department and hour, count incidents\n",
    "    dept_hour_counts = dept_time_filtered.groupby(['department', 'hour']).size().reset_index(name='count')\n",
    "    \n",
    "    # Plot line chart\n",
    "    sns.lineplot(data=dept_hour_counts, x='hour', y='count', hue='department', marker='o')\n",
    "    plt.title('Incident Counts by Hour and Department')\n",
    "    plt.xlabel('Hour of Day')\n",
    "    plt.ylabel('Number of Incidents')\n",
    "    plt.xticks(range(0, 24, 2))  # Show every other hour for readability\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"cleaned_data/final/imgs/extended_eda/department_hour_distribution.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    # Create a heatmap of departments by hour\n",
    "    dept_hour_pivot = dept_hour_counts.pivot(index='department', columns='hour', values='count')\n",
    "    dept_hour_pivot = dept_hour_pivot.fillna(0)\n",
    "    \n",
    "    plt.figure(figsize=(14, 8))\n",
    "    sns.heatmap(dept_hour_pivot, cmap='YlOrRd', annot=True, fmt='.0f')\n",
    "    plt.title('Incident Heatmap by Department and Hour')\n",
    "    plt.xlabel('Hour of Day')\n",
    "    plt.ylabel('Department')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"cleaned_data/final/imgs/extended_eda/department_hour_heatmap.png\")\n",
    "    plt.close()\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# 5. Time, Violence Type, and Severity Analysis\n",
    "# -----------------------------------------------------------------------\n",
    "print(\"Analyzing violence patterns by time of day...\")\n",
    "\n",
    "if all(col in df.columns for col in ['violence_type', 'emotional_impact_ML']) and 'hour' in time_features.columns:\n",
    "    # Create a combined dataframe\n",
    "    time_violence_df = df.join(time_features).dropna(subset=['violence_type', 'emotional_impact_ML', 'hour'])\n",
    "    \n",
    "    # Group by hour and violence type, count incidents\n",
    "    hour_violence_counts = time_violence_df.groupby(['hour', 'violence_type']).size().reset_index(name='count')\n",
    "    \n",
    "    # Plot line chart\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    sns.lineplot(data=hour_violence_counts, x='hour', y='count', hue='violence_type', marker='o')\n",
    "    plt.title('Violence Type by Hour of Day')\n",
    "    plt.xlabel('Hour of Day')\n",
    "    plt.ylabel('Number of Incidents')\n",
    "    plt.xticks(range(0, 24, 2))\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"cleaned_data/final/imgs/extended_eda/violence_type_hour.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    # Create a heatmap of emotional impact severity by hour\n",
    "    # Calculate proportion of severe incidents by hour\n",
    "    severe_by_hour = time_violence_df.groupby('hour')['emotional_impact_ML'].apply(\n",
    "        lambda x: (x == 'Severe').mean()\n",
    "    ).reset_index(name='severe_proportion')\n",
    "    \n",
    "    plt.figure(figsize=(14, 6))\n",
    "    sns.barplot(data=severe_by_hour, x='hour', y='severe_proportion')\n",
    "    plt.title('Proportion of Severe Emotional Impact by Hour')\n",
    "    plt.xlabel('Hour of Day')\n",
    "    plt.ylabel('Proportion of Severe Cases')\n",
    "    plt.xticks(range(0, 24))\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"cleaned_data/final/imgs/extended_eda/severe_impact_by_hour.png\")\n",
    "    plt.close()\n",
    "\n",
    "print(\"Extended EDA analysis complete!\")\n",
    "\n",
    "# Continue with standard data preprocessing\n",
    "print(\"Preprocessing data...\")\n",
    "target = 'emotional_impact_ML'\n",
    "text_col = 'assault_desc'\n",
    "struct_features = ['victim_profession', 'department', 'perpetrator_type', 'violence_type', 'emotional_impact', 'response_action']\n",
    "df = df.dropna(subset=struct_features + [target, text_col])\n",
    "\n",
    "# Encode target variable\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(df[target])\n",
    "target_names = label_encoder.classes_\n",
    "print(f\"Target classes: {target_names}\")\n",
    "\n",
    "# Structured feature encoding\n",
    "X_struct = pd.get_dummies(df[struct_features])\n",
    "\n",
    "# Text feature processing\n",
    "tfidf = TfidfVectorizer(max_features=300, stop_words='english')\n",
    "X_text = tfidf.fit_transform(df[text_col])\n",
    "print(f\"TF-IDF extracted {X_text.shape[1]} text features\")\n",
    "\n",
    "# 2. Combine all features (improved feature combination approach)\n",
    "print(\"Combining features...\")\n",
    "feature_matrices = [X_struct]\n",
    "feature_names = list(X_struct.columns)\n",
    "\n",
    "# Add text features\n",
    "feature_matrices.append(X_text)\n",
    "feature_names.extend(tfidf.get_feature_names_out())\n",
    "\n",
    "\n",
    "\n",
    "# Fix for converting time features to sparse matrix\n",
    "if 'hour' in time_features.columns:\n",
    "    # Get numeric columns only\n",
    "    numeric_cols = time_features.select_dtypes(include=[np.number]).columns\n",
    "    categorical_cols = time_features.select_dtypes(exclude=[np.number]).columns\n",
    "    \n",
    "    # Handle numeric features\n",
    "    numeric_features = time_features[numeric_cols].reindex(df.index, fill_value=0)\n",
    "    feature_matrices.append(csr_matrix(numeric_features.values))\n",
    "    feature_names.extend(numeric_cols)\n",
    "    \n",
    "    # Handle categorical features - we need to one-hot encode these\n",
    "    for cat_col in categorical_cols:\n",
    "        # Skip if already one-hot encoded\n",
    "        if cat_col != 'day_night':  # 'day_night' should already be one-hot encoded before this point\n",
    "            cat_dummies = pd.get_dummies(time_features[cat_col], prefix=cat_col)\n",
    "            cat_dummies_aligned = cat_dummies.reindex(df.index, fill_value=0)\n",
    "            feature_matrices.append(csr_matrix(cat_dummies_aligned.values))\n",
    "            feature_names.extend(cat_dummies_aligned.columns)\n",
    "    \n",
    "    print(f\"Added time features to the model\")\n",
    "# Merge all features\n",
    "X_combined = hstack(feature_matrices)\n",
    "X_combined = StandardScaler(with_mean=False).fit_transform(X_combined)\n",
    "print(f\"Final feature matrix shape: {X_combined.shape}\")\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_combined, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Apply SMOTE to handle class imbalance\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
    "print(f\"Training set shape after SMOTE: {X_train_res.shape}\")\n",
    "\n",
    "# 3. Enhanced model training (added class weights and cross-validation)\n",
    "print(\"Training and evaluating models...\")\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000, class_weight='balanced'),\n",
    "    \"Random Forest\": RandomForestClassifier(class_weight='balanced', n_estimators=100),\n",
    "    \"SVM\": SVC(probability=True, class_weight='balanced')\n",
    "}\n",
    "\n",
    "# Evaluate and save confusion matrices and ROC curves\n",
    "model_scores = {}\n",
    "labels = sorted(df[target].unique())\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name} model...\")\n",
    "    \n",
    "    # Evaluate model performance using cross-validation\n",
    "    cv_scores = cross_val_score(model, X_train_res, y_train_res, cv=5, scoring='f1_weighted')\n",
    "    print(f\"{name} - Cross-Validation Score (F1 weighted): {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")\n",
    "    \n",
    "    # Train on the full training set\n",
    "    model.fit(X_train_res, y_train_res)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Get classification report\n",
    "    report = classification_report(y_test, y_pred, target_names=labels, output_dict=True)\n",
    "    model_scores[name] = report[\"weighted avg\"]\n",
    "    \n",
    "    print(f\"\\n{name} - Classification Report\")\n",
    "    print(classification_report(y_test, y_pred, target_names=labels))\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    disp.plot(cmap=\"Blues\")\n",
    "    plt.title(f\"{name} - Emotional Impact Classification\")\n",
    "    plt.savefig(f\"cleaned_data/final/imgs/{name.replace(' ', '_').lower()}_cm.png\", bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # 4. Calculate and plot ROC curves (new addition)\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        print(f\"Generating ROC curves for {name}...\")\n",
    "        # Get prediction probabilities\n",
    "        y_prob = model.predict_proba(X_test)\n",
    "        \n",
    "        # Calculate ROC and AUC for multiclass\n",
    "        fpr = dict()\n",
    "        tpr = dict()\n",
    "        roc_auc = dict()\n",
    "        \n",
    "        # Prepare one-vs-rest classifier to calculate ROC for each class\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        \n",
    "        n_classes = len(labels)\n",
    "        for i in range(n_classes):\n",
    "            # Use current class as positive, others as negative\n",
    "            y_test_binary = (y_test == i).astype(int)\n",
    "            y_score = y_prob[:, i]\n",
    "            \n",
    "            fpr[i], tpr[i], _ = roc_curve(y_test_binary, y_score)\n",
    "            roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "            \n",
    "            plt.plot(fpr[i], tpr[i], lw=2,\n",
    "                    label=f'{labels[i]} (AUC = {roc_auc[i]:.2f})')\n",
    "        \n",
    "        # Calculate average ROC curve\n",
    "        all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "        mean_tpr = np.zeros_like(all_fpr)\n",
    "        for i in range(n_classes):\n",
    "            mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
    "        mean_tpr /= n_classes\n",
    "        \n",
    "        mean_auc = auc(all_fpr, mean_tpr)\n",
    "        plt.plot(all_fpr, mean_tpr, 'k--',\n",
    "                label=f'Average ROC (AUC = {mean_auc:.2f})', lw=2)\n",
    "        \n",
    "        plt.plot([0, 1], [0, 1], 'r--', lw=2)\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title(f'ROC Curve - {name}')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.savefig(f\"cleaned_data/final/imgs/{name.replace(' ', '_').lower()}_roc.png\", bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "# 5. Enhanced feature importance analysis\n",
    "print(\"Analyzing feature importance...\")\n",
    "if \"Random Forest\" in models:\n",
    "    rf_model = models[\"Random Forest\"]\n",
    "    importances = rf_model.feature_importances_\n",
    "    \n",
    "    # Expanded to top 15\n",
    "    indices = np.argsort(importances)[-15:][::-1]  \n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(x=importances[indices], y=np.array(feature_names)[indices])\n",
    "    plt.title(\"Top 15 Feature Importances (Random Forest)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"cleaned_data/final/imgs/rf_feature_importance_extended.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    # Analyze importance of time features\n",
    "    time_feature_indices = [i for i, name in enumerate(feature_names) \n",
    "                           if any(time_prefix in name for time_prefix in \n",
    "                                 ['hour', 'day_of_week', 'month', 'is_weekend', 'day_night'])]\n",
    "    \n",
    "    if time_feature_indices:\n",
    "        time_importances = importances[time_feature_indices]\n",
    "        time_names = np.array(feature_names)[time_feature_indices]\n",
    "        \n",
    "        # Sort by importance\n",
    "        time_sorted_idx = np.argsort(time_importances)[::-1]\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.barplot(x=time_importances[time_sorted_idx], y=time_names[time_sorted_idx])\n",
    "        plt.title(\"Time-Based Feature Importance\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"cleaned_data/final/imgs/time_feature_importance.png\")\n",
    "        plt.close()\n",
    "        print(\"Generated time feature importance plot\")\n",
    "\n",
    "# SHAP analysis (enhanced)\n",
    "print(\"Performing SHAP analysis...\")\n",
    "X_train_sample = X_train_res[:500].toarray()  # Use larger sample for better explainability\n",
    "X_test_sample = X_test[:100].toarray()\n",
    "\n",
    "explainer = shap.Explainer(rf_model, X_train_sample)\n",
    "shap_values = explainer(X_test_sample)\n",
    "\n",
    "# Basic SHAP summary plot\n",
    "shap.summary_plot(shap_values, X_test_sample, feature_names=feature_names, max_display=15, show=False)\n",
    "plt.title(\"SHAP Summary - Random Forest (Extended)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"cleaned_data/final/imgs/shap_summary_extended.png\", bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# SHAP summary plots for each class\n",
    "for i, label in enumerate(labels):\n",
    "    if shap_values.shape[1] > i:  # Ensure we have enough classes\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        shap.summary_plot(shap_values[:, :, i], X_test_sample, feature_names=feature_names, \n",
    "                         max_display=10, show=False)\n",
    "        plt.title(f\"SHAP Summary - Features influencing {label} prediction\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"cleaned_data/final/imgs/shap_summary_{label.lower()}.png\", bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"Generated SHAP analysis plot for {label} class\")\n",
    "\n",
    "# Model comparison summary\n",
    "print(\"\\n\\nModel Comparison Summary (Weighted F1, Precision, Recall):\")\n",
    "for model_name, metrics in model_scores.items():\n",
    "    print(f\"{model_name}: F1={metrics['f1-score']:.2f}, Precision={metrics['precision']:.2f}, Recall={metrics['recall']:.2f}\")\n",
    "\n",
    "# 6. Visualize model comparison (new addition)\n",
    "plt.figure(figsize=(10, 6))\n",
    "metrics = ['f1-score', 'precision', 'recall']\n",
    "model_names = list(model_scores.keys())\n",
    "x = np.arange(len(model_names))\n",
    "width = 0.2\n",
    "multiplier = 0\n",
    "\n",
    "for metric in metrics:\n",
    "    metric_values = [model_scores[model][metric] for model in model_names]\n",
    "    offset = width * multiplier\n",
    "    plt.bar(x + offset, metric_values, width, label=metric)\n",
    "    multiplier += 1\n",
    "\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Model Performance Comparison')\n",
    "plt.xticks(x + width, model_names)\n",
    "plt.legend(loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"cleaned_data/final/imgs/model_comparison.png\")\n",
    "plt.close()\n",
    "print(\"Generated model comparison chart\")\n",
    "\n",
    "# KMeans clustering analysis (original part)\n",
    "print(\"Performing KMeans clustering analysis...\")\n",
    "cluster_cols = ['victim_profession', 'department', 'violence_type']\n",
    "cluster_df = df.dropna(subset=cluster_cols)\n",
    "X_cluster = pd.get_dummies(cluster_df[cluster_cols])\n",
    "\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "X_pca = pca.fit_transform(X_cluster)\n",
    "\n",
    "kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
    "clusters = kmeans.fit_predict(X_cluster)\n",
    "\n",
    "pca_df = pd.DataFrame(X_pca, columns=[\"PC1\", \"PC2\"])\n",
    "pca_df[\"Cluster\"] = clusters\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(data=pca_df, x=\"PC1\", y=\"PC2\", hue=\"Cluster\", palette=\"Set2\")\n",
    "plt.title(\"KMeans Clustering of Roles & Violence Types (PCA 2D)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"cleaned_data/final/imgs/kmeans_pca_roles.png\")\n",
    "plt.close()\n",
    "\n",
    "# Cluster summaries\n",
    "cluster_df['Cluster'] = clusters\n",
    "for col in cluster_cols:\n",
    "    print(f\"\\nTop 3 '{col}' values per cluster:\")\n",
    "    print(cluster_df.groupby('Cluster')[col].value_counts().groupby(level=0).nlargest(3))\n",
    "\n",
    "# 7. Time feature analysis summary (new addition)\n",
    "print(\"\\n\\nTime Feature Analysis:\")\n",
    "print(\"=======================\")\n",
    "if 'hour' in time_features.columns and valid_dates.any():\n",
    "    # Analyze severity by time of day\n",
    "    hour_severity = {}\n",
    "    valid_hours = df.loc[valid_dates, 'event_time'].dt.hour\n",
    "    valid_severity = df.loc[valid_dates, target]\n",
    "    \n",
    "    for hour in range(24):\n",
    "        hour_data = valid_severity[valid_hours == hour]\n",
    "        if not hour_data.empty:\n",
    "            severity_counts = hour_data.value_counts(normalize=True)\n",
    "            hour_severity[hour] = severity_counts.to_dict()\n",
    "    \n",
    "    # Print time-related insights\n",
    "    print(\"Hours with highest severe incident rates:\")\n",
    "    severe_rates = {hour: data.get('Severe', 0) for hour, data in hour_severity.items() if 'Severe' in data}\n",
    "    for hour, rate in sorted(severe_rates.items(), key=lambda x: x[1], reverse=True)[:5]:\n",
    "        print(f\"Hour {hour}: {rate:.2%}\")\n",
    "    \n",
    "    print(\"\\nWeekday vs. Weekend incident severity comparison:\")\n",
    "    weekday_data = valid_severity[df.loc[valid_dates, 'event_time'].dt.dayofweek < 5]\n",
    "    weekend_data = valid_severity[df.loc[valid_dates, 'event_time'].dt.dayofweek >= 5]\n",
    "    \n",
    "    print(\"Weekday severity distribution:\")\n",
    "    print(weekday_data.value_counts(normalize=True))\n",
    "    \n",
    "    print(\"\\nWeekend severity distribution:\")\n",
    "    print(weekend_data.value_counts(normalize=True))\n",
    "else:\n",
    "    print(\"Time features could not be extracted from the dataset.\")\n",
    "\n",
    "print(\"\\nAnalysis complete! All charts and results have been saved to the 'cleaned_data/final/imgs/' directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe072215-d6f7-4a4d-949d-a136ee781f02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
